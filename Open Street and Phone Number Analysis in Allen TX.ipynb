{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Street and Phone Number Analysis in Allen, TX \n",
    "\n",
    "### Map Area: Allen, TX  \n",
    "\n",
    "**Motivation** \n",
    "\n",
    "\n",
    "The reason why I am choosing Allen in Texas is because I am considering living there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cerberus in ./anaconda2/lib/python2.7/site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: schema in ./anaconda2/lib/python2.7/site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Importing needed libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import csv\n",
    "import codecs\n",
    "import pip\n",
    "\n",
    "def install(package):\n",
    "   pip.main(['install', package])\n",
    "\n",
    "install('cerberus')\n",
    "\n",
    "import cerberus\n",
    "\n",
    "def install(package):\n",
    "   pip.main(['install', package])\n",
    "\n",
    "install('schema')\n",
    "# Do the import\n",
    "import schema as sch\n",
    "\n",
    "\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSM file import\n",
    "osm_file = \"ex_ijn8icTrVTwhvS28Nvi9uBAa3A7aQ.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSM file contains all the data from Open Map Street."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re function\n",
    "# Compile a regular expression pattern into a regular expression object\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# Expected street types list\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\",\n",
    "            \"Square\", \"Lane\", \"Road\", \"Trail\", \"Parkway\", \"Commons\", \"Alley\",\n",
    "            \"Bridge\", \"Highway\", \"Circle\", \"Terrace\", \"Way\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to identify here above the regular expressions. We are listing all the possible street types that could be contained in the OSM file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding street names in dictionary by type\n",
    "# Takes 2 arguments: dictionary and string. If string doesn't match pattern adds it to dictionary.\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we added the street name in the dictionary by type. If there is no match, those names are added to a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking tag for street data content\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we are checking the street data content from the OSM file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we are auditing the file. This will consist of reading the file and parsing it, taking the osm file as input and returning dictionary of different abbreviations of different street types.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_types = audit(osm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, the audit function we created is being executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'110': set(['Greenville Ave #110']),\n",
      " '121': set(['SH 121', 'State Hwy 121', 'TX 121']),\n",
      " '300': set(['Collin McKinney Pkwy, Suite 300']),\n",
      " '510': set(['E Stacy Road Ste 510']),\n",
      " 'Blvd': set(['Chase Oaks Blvd']),\n",
      " 'Blvd.': set(['Hardin Blvd.']),\n",
      " 'Cove': set(['Pine Bluff Cove']),\n",
      " 'Dr': set(['Lynbridge Dr', 'McDermott Dr', 'Red River Dr', 'Woodson Dr']),\n",
      " 'Expessway': set(['South Central Expessway']),\n",
      " 'Expressway': set(['Central Expressway',\n",
      "                    'North Central Expressway',\n",
      "                    'S Central Expressway',\n",
      "                    'South Central Expressway']),\n",
      " 'Landing': set(['Kemps Landing']),\n",
      " 'McDermott': set(['McDermott']),\n",
      " 'Point': set(['Lookout Point']),\n",
      " 'Rd': set(['Jupiter Rd', 'McDermott Rd']),\n",
      " 'S': set(['S']),\n",
      " 'South': set(['Central Expressway South']),\n",
      " 'St': set(['E Main St']),\n",
      " 'Tollway': set(['Sam Rayburn Tollway']),\n",
      " 'Trace': set(['Natchez Trace'])}\n"
     ]
    }
   ],
   "source": [
    "# Printing results\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we're printing the different street type. There are some streets that are not completely defined. There is a consistency issue here. For example, in the cell above, we have Blvd and Blvd. even though they both mean the same definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a mapping of abbreviation to their complete names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"St\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"ST\": \"Street\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"Ave.\": \"Avenue\",\n",
    "           \"Av.\": \"Avenue\",\n",
    "           \"Av\": \"Avenue\",\n",
    "           \"Sq\": \"Square\",\n",
    "           \"CT\": \"Court\",\n",
    "           \"Ct\": \"Court\",\n",
    "           \"DR\": \"Drive\",\n",
    "           \"Dr\": \"Drive\",\n",
    "           \"Dr.\": \"Drive\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"Pl\": \"Place\",\n",
    "           \"Hwy\": \"Highway\",\n",
    "           \"Ln\": \"Lane\",\n",
    "           \"Blvd\": \"Boulevard\",\n",
    "           \"Blvd.\": \"Boulevard\",\n",
    "           \"Brdg\": \"Bridge\",\n",
    "           \"Ter\": \"Terrace\"\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_file_2 = \"AllenTexas2.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we are creating a resulting file from the mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following function replace abbreviation by full name in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    words = name.split(' ')\n",
    "    last_word = words[-1]\n",
    "    if last_word in mapping.keys():\n",
    "        name2 = name.replace(last_word, mapping[last_word])\n",
    "        return name2\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, the function takes OSM file as imput and yields nodes of type from tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_street(old_file, new_file):\n",
    "    with open(new_file, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')\n",
    "        for i, element in enumerate(get_element(old_file)):\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    tag.set('v', update_name(tag.attrib['v'], mapping))\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "        output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, the function creates the modification of the origional OSM file by mapping the abbreviation and writing the result in a new file called osm_file_2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying osm file\n",
    "modify_street(osm_file, osm_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function above is being executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking tag for postcode content\n",
    "zip_type_re = re.compile(r'\\d{5}-??')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell above is used to check tag for post code contents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to sort the zip codes and save them into a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting zipcodes in different forms and save them to dict\n",
    "def audit_zip_type(zip_types, zip):\n",
    "    m = zip_type_re.search(zip)\n",
    "    if m:\n",
    "        zip_type = m.group()\n",
    "        if zip_type not in zip_types:\n",
    "            zip_types[zip_type].add(zip)\n",
    "    else:\n",
    "        zip_types['unknown'].add(zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if tag contains postcode\n",
    "def is_zip(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function checks if the tags contain postcodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create a function that audits the file based on zipcodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auditing file for different variations of same zip\n",
    "def zip_audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    zip_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zip(tag):\n",
    "                    audit_zip_type(zip_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return zip_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip code audit is now being executed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zp_types = zip_audit(osm_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results from the zip code audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'75002': set(['75002']),\n",
      " '75013': set(['75013']),\n",
      " '75023': set(['75023']),\n",
      " '75025': set(['75025']),\n",
      " '75069': set(['75069']),\n",
      " '75070': set(['75070']),\n",
      " '75074': set(['75074'])}\n"
     ]
    }
   ],
   "source": [
    "# Printing results\n",
    "pprint.pprint(dict(zp_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, for the sample we have above, the zip codes are unique and seem to have reasonable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_file_3 = \"AllenTexas3.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we're creating a new file container which will get the results from the zip code modification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is going to update the wrong zip codes and replace them with the right zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_zip(zip):\n",
    "    m = zip_type_re.search(zip)\n",
    "    if m:\n",
    "        return m.group()\n",
    "    else:\n",
    "        return 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_zip(old_file, new_file):\n",
    "    with open(new_file, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')\n",
    "        for i, element in enumerate(get_element(old_file)):\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_zip(tag):\n",
    "                    tag.set('v', update_zip(tag.attrib['v']))\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "        output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to execute the zip code modification and write the results in osm_file_3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_zip(osm_file_2, osm_file_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to audit the phone information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the tag for phone content\n",
    "phone_type_re = re.compile(r'\\d{3}\\)?-?\\s?.?\\d{3}\\s?-?\\s?.?\\d{4}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we're checking the type for phone contents. Let's compile any phone number in bad format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_re = re.compile('\\.|\\)|\\s|-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our phone numbers to start with 1 and have an area code. We also want our phone numbers to have 10 digits in total ranging from 0 to 9 to be valid. The function below addresses this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_phone_type(phone_types, phone):\n",
    "    m = phone_type_re.search(phone)\n",
    "    if m:\n",
    "        phone_type = m.group()\n",
    "        if phone_type not in phone_types:\n",
    "            new_phone = phone_re.sub('', phone_type)\n",
    "            new_phone = ('+1-' + new_phone[:3] + '-' +\n",
    "                         new_phone[3:6] + '-' + new_phone[6:])\n",
    "            phone_types[new_phone].add(phone)\n",
    "    else:\n",
    "        phone_types['unknown'].add(phone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us check if the tag contains \"phone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if tag contains phone\n",
    "def is_phone(elem):\n",
    "    return (elem.attrib['k'] == \"phone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now audit the phone information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    phone_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_phone(tag):\n",
    "                    audit_phone_type(phone_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return phone_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now execute the phone audit function created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_types = phone_audit(osm_file_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now print the results of the phone audit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'+1-214-383-3500': set(['214-383-3500']),\n",
      " '+1-214-383-4008': set(['+1214-383-4008']),\n",
      " '+1-214-383-5353': set(['(214) 383-5353']),\n",
      " '+1-214-383-9712': set(['+1 214 383 9712']),\n",
      " '+1-214-509-4653': set(['+1 214 509 4653']),\n",
      " '+1-214-644-0280': set(['(214) 644-0280']),\n",
      " '+1-214-667-2100': set(['+1 214 667 2100']),\n",
      " '+1-469-752-0600': set(['+1 (469) 752-0600']),\n",
      " '+1-469-854-6681': set(['469-854-6681']),\n",
      " '+1-972-363-2101': set(['+1 972 363 2101']),\n",
      " '+1-972-390-1040': set(['+1 972 390 1040']),\n",
      " '+1-972-390-9917': set(['(972) 390-9917']),\n",
      " '+1-972-578-9779': set(['+1-972-578-9779']),\n",
      " '+1-972-649-6913': set(['(972) 649-6913']),\n",
      " '+1-972-678-4700': set(['972-678-4700']),\n",
      " '+1-972-678-7000': set(['+1 972-678-7000']),\n",
      " '+1-972-727-0250': set(['+1 972 727 0250']),\n",
      " '+1-972-912-1097': set(['+1 972 912 1097'])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(ph_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that phone numbers do not start with 1 that identifies US country code. Other phone numbers do not separate the digit 1 from the other numbers. Other numbers are with or without dashes. There is a problem of consistency here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us update the format of these phone numbers contained in the OSM file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, let's a create a new data container called OSM_file_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_file_4 = \"AllenTexas4.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'+1-214-383-3500': set(['214-383-3500']),\n",
      " '+1-214-383-4008': set(['+1214-383-4008']),\n",
      " '+1-214-383-5353': set(['(214) 383-5353']),\n",
      " '+1-214-383-9712': set(['+1 214 383 9712']),\n",
      " '+1-214-509-4653': set(['+1 214 509 4653']),\n",
      " '+1-214-644-0280': set(['(214) 644-0280']),\n",
      " '+1-214-667-2100': set(['+1 214 667 2100']),\n",
      " '+1-469-752-0600': set(['+1 (469) 752-0600']),\n",
      " '+1-469-854-6681': set(['469-854-6681']),\n",
      " '+1-972-363-2101': set(['+1 972 363 2101']),\n",
      " '+1-972-390-1040': set(['+1 972 390 1040']),\n",
      " '+1-972-390-9917': set(['(972) 390-9917']),\n",
      " '+1-972-578-9779': set(['+1-972-578-9779']),\n",
      " '+1-972-649-6913': set(['(972) 649-6913']),\n",
      " '+1-972-678-4700': set(['972-678-4700']),\n",
      " '+1-972-678-7000': set(['+1 972-678-7000']),\n",
      " '+1-972-727-0250': set(['+1 972 727 0250']),\n",
      " '+1-972-912-1097': set(['+1 972 912 1097'])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(ph_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates format of phone numbers to right one.\n",
    "def update_phone(phone):\n",
    "    m = phone_type_re.search(phone)\n",
    "    if m:\n",
    "        new_phone = phone_re.sub('', m.group())\n",
    "        return ('+1-' + new_phone[:3] + '-' + new_phone[3:6] +\n",
    "                '-' + new_phone[6:])\n",
    "    else:\n",
    "        return phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function replace wrong phone formats in osm file\n",
    "def modify_phone(old_file, new_file):\n",
    "    with open(new_file, 'wb') as output:\n",
    "        output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "        output.write('<osm>\\n  ')\n",
    "        for i, element in enumerate(get_element(old_file)):\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if is_phone(tag):\n",
    "                    tag.set('v', update_phone(tag.attrib['v']))\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "        output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will update the wrong phone information with the right one\n",
    "modify_phone(osm_file_3, osm_file_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to create the csv files results and embbed them into sql format. We will then use sqlite3 to embbed those files in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to new csv files\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression compilers\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCH = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The schema is stored in a .py file in order to take advantage of the\n",
    "# int() and float() type coercion functions. Otherwise it could easily stored as\n",
    "#as JSON or another serialized format.\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing schema for pransformation from schema.py file\n",
    "SCHEMA = sch.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields of new csv files\n",
    "#The schema describes what fields will get into the database and their format\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version',\n",
    "               'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for transformation of XML data to Python dict\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS,\n",
    "                  way_attr_fields=WAY_FIELDS, prob_ch=PROBLEMCH,\n",
    "                  default_tag_type='regular'):\n",
    "    tag_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "    count = 0\n",
    "    if element.tag == 'node':\n",
    "        tagfields = node_attr_fields\n",
    "    elif element.tag == 'way':\n",
    "        tagfields = way_attr_fields\n",
    "\n",
    "    if element.tag == 'node' or 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in tagfields:\n",
    "                tag_attribs[attrib] = element.attrib[attrib]\n",
    "    for subelem in element:\n",
    "        if subelem.tag == 'tag' and prob_ch.match(subelem.attrib['k']) == None:\n",
    "            tag = {}\n",
    "            tag['id'] = tag_attribs['id']\n",
    "            tag['value'] = subelem.attrib['v']\n",
    "            key = subelem.attrib['k']\n",
    "            tag['key'] = key[key.find(':') + 1:]\n",
    "            if ':' in key:\n",
    "                tag['type'] = key[:key.find(':')]\n",
    "            else:\n",
    "                tag['type'] = default_tag_type\n",
    "            tags.append(tag)\n",
    "        elif subelem.tag == 'nd':\n",
    "            way_node = {}\n",
    "            way_node['id'] = tag_attribs['id']\n",
    "            way_node['node_id'] = subelem.attrib['ref']\n",
    "            way_node['position'] = count\n",
    "            count += 1\n",
    "            way_nodes.append(way_node)\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        return {'node': tag_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': tag_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're validating element to match schema. This function validate every element to match the schema.\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend csv.DictWriter to handle Unicode input\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "                                                    k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in\n",
    "                                                    row.iteritems()\n",
    "                                                    })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function processing osm file to 5 csv files. This function takes the osm file result and produces\n",
    "# the csv files as final results\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "            codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "            codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "            codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "            codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us now process the osm file and get the results\n",
    "process_map(osm_file_4, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was successfully processed and the output file was generated. The output file is included in the submission and it contains the filtered and corrected map data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the jupyter notebook file for this project is 29 KB. \n",
    "There are 116849 of nodes in our dataset and there are 15424 ways. There are 444067616996  nodes in the biggest way in this database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The above results were generated by sql queries which are provided below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries we used are below: \n",
    "\n",
    "BUILDING SQL DB BY SCHEMA  \n",
    "\n",
    "In this section we will build empty SQL database and create table schema.\n",
    "We'll use sqlite3 shell for this purpose. It's needed to create DB and read create_db.csv file. Next command will execute create_csv.db file by executing next command:\n",
    "\n",
    ".read create_csv.db\n",
    "\n",
    "This file contains next rows:\n",
    "\n",
    "CREATE TABLE nodes ( id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT );\n",
    "CREATE TABLE nodes_tags ( id INTEGER, key TEXT, value TEXT, type TEXT, FOREIGN KEY (id) REFERENCES nodes(id) );\n",
    "CREATE TABLE ways ( id INTEGER PRIMARY KEY NOT NULL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp TEXT );\n",
    "CREATE TABLE ways_tags ( id INTEGER NOT NULL, key TEXT NOT NULL, value TEXT NOT NULL, type TEXT, FOREIGN KEY (id) REFERENCES ways(id) );\n",
    "CREATE TABLE ways_nodes ( id INTEGER NOT NULL, node_id INTEGER NOT NULL, position INTEGER NOT NULL, FOREIGN KEY (id) REFERENCES ways(id), FOREIGN KEY (node_id) REFERENCES nodes(id) );\n",
    "   .mode csv\n",
    "   .import nodes.csv nodes\n",
    "   .import ways.csv ways\n",
    "   .import nodes_tags.csv nodes_tags\n",
    "    delete from nodes_tags where id = 'id';\n",
    "    .import ways_tags.csv ways_tags\n",
    "    delete from ways_tags where id = 'id';\n",
    "    .import ways_nodes.csv ways_nodes\n",
    "   delete from ways_nodes where id = 'id';\n",
    "\n",
    "\n",
    "SQLite3 QUERING \n",
    "\n",
    "Establishing connection and cursor\n",
    "conn = sqlite3.connect(\"osm.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "Executing and printing\n",
    "cursor.execute(\"select count(id) from nodes;\")\n",
    "print 'There are {} nodes in database.'.format(cursor.fetchall()[0][0])\n",
    "\n",
    "cursor.execute(\"select count(id) from ways;\")\n",
    "print 'There are {} ways in database.'.format(cursor.fetchall()[0][0])\n",
    "\n",
    "cursor.execute(\"select count(distinct(uid)) from (select uid from nodes union select uid from ways);\")\n",
    "print 'There are {} uniqe users in database.'.format(cursor.fetchall()[0][0])\n",
    "\n",
    "cursor.execute(\"select id, count(*) as nodes_count from ways_nodes group by id order by nodes_count desc limit 1;\")\n",
    "way_id, count = cursor.fetchall()[0]\n",
    "print \"There're {} nodes in the biggest way in database. Way id is {}.\".format(count, way_id)\n",
    "\n",
    "cursor.execute(\"select * from ways_tags where id = {};\".format(way_id))\n",
    "print 'This way is:'\n",
    "pprint.pprint(cursor.fetchall())\n",
    "\n",
    "cursor.execute(\"select count(key) from ways_tags where key = 'bridge' and value != 'yes' group by key;\")\n",
    "print \"There are {} bridges in Pitt. That's a second Venice.\".format(cursor.fetchall()[0][0])\n",
    "\n",
    "cursor.execute(\"select value, count(*) as count from nodes_tags where key = 'postcode' group by value order by count desc limit 5;\")\n",
    "pprint.pprint(cursor.fetchall())\n",
    "\n",
    "\n",
    "sqlite3 nodes.db\n",
    "CREATE TABLE nodes ( id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One suggestion for improving this analysis is to run the queries from within the jupyter environment because this would shorten the number of steps required to count the nodes and ways. \n",
    "We're setting up a database and doing some queries from within Jupyter here: One benefit in doing these following steps below would be that it allow us to count the nodes faster and more efficiently. One possible problem is that the syntax may differ for some of the queries (excluding the one below). \n",
    "\n",
    "\n",
    "conn = sqlite3.connect('example.db')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE nodes ( id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT );''')\n",
    "c.execute('''CREATE TABLE nodes_tags ( id INTEGER, key TEXT, value TEXT, type TEXT, FOREIGN KEY (id) REFERENCES nodes(id) );''')\n",
    "c.execute('''CREATE TABLE ways ( id INTEGER PRIMARY KEY NOT NULL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp TEXT );''')\n",
    "c.execute('''CREATE TABLE ways_tags ( id INTEGER NOT NULL, key TEXT NOT NULL, value TEXT NOT NULL, type TEXT, FOREIGN KEY (id) REFERENCES ways(id) );''')\n",
    "c.execute('''CREATE TABLE ways_nodes ( id INTEGER NOT NULL, node_id INTEGER NOT NULL, position INTEGER NOT NULL, FOREIGN KEY (id) REFERENCES ways(id), FOREIGN KEY (node_id) REFERENCES nodes(id) );''')\n",
    "c.execute('''mode csv''')\n",
    "c.execute('''import nodes.csv nodes''')\n",
    "c.execute('''import ways.csv ways''')\n",
    "c.execute('''import nodes_tags.csv nodes_tags''')\n",
    "c.execute('''delete from nodes_tags where id = 'id';''')\n",
    "c.execute('''import ways_tags.csv ways_tags''')\n",
    "c.execute('''delete from ways_tags where id = 'id';''')\n",
    "c.execute('''import ways_nodes.csv ways_nodes''')\n",
    "c.execute('''delete from ways_nodes where id = 'id';''')\n",
    "\n",
    "One possible way in improving our data set in the future may be that in this analysis, it was discovered that in many instances, street names were abbreviated in multiple ways and the abbreviations had to be processed and filtered. For example, drive was originally inconsistently named such as DR, Dr, and Dr. Thus, we needed to filter those three terms as Drive in order for the code to run. \n",
    "\n",
    "Another issue we had was that there were 22 phone numbers in which 18 of them were inconsistent (i.e. there were extra plus signs, minus signs, inconsistent spacing, etc.) and we had to clean up these 18 numbers in the right format. In the future, in order to improve our data to be more consistent, we can automatically format the user input as soon as they are entered and before they are saved in such a way that they cannot enter the phone numbers in a wrong format. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
